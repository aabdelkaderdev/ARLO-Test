fastapi>=0.104.0
uvicorn[standard]>=0.24.0
pydantic>=2.5.0
pydantic-settings>=2.1.0
httpx>=0.25.0
python-dotenv>=1.0.0
scikit-learn>=1.3.0
numpy>=1.26.0
ortools>=9.8.3296
gradio>=4.0.0
weasyprint>=60.0
jinja2>=3.1.0

# New dependencies for vLLM support
torch>=2.0.0
sentence-transformers>=2.2.0

# Note: vllm requires specific CUDA version. Install separately:
# pip install vllm>=0.4.0
# Or for specific CUDA version:
# pip install vllm --extra-index-url https://download.pytorch.org/whl/cu121
